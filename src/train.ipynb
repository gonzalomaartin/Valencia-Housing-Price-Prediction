{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593fb1f6",
   "metadata": {},
   "source": [
    "#### This file is going to deal with training a model to predict property price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c010fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from xgboost import XGBRegressor \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer \n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1aec76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vlc = pd.read_csv(\"../working_data/properties_vlc_clean2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d99d7a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_vlc.drop(columns = [\"price\"])\n",
    "y = df_vlc[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6eb3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_columns = []\n",
    "one_hot_columns = [\"location_cluster\"]\n",
    "integer_columns = []\n",
    "\n",
    "for column in df_vlc.columns: \n",
    "    if column == \"price\": \n",
    "        continue \n",
    "    elif df_vlc[column].dtype == \"int64\": \n",
    "        if column not in one_hot_columns: \n",
    "            integer_columns.append(column)\n",
    "    elif df_vlc[column].dtype == \"bool\": \n",
    "        boolean_columns.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e64256b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vlc = pd.get_dummies(df_vlc, columns = [\"location_cluster\"], prefix = \"location\", drop_first = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5410768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing feature matrix...\n",
      "Feature matrix shape: (15726, 108)\n",
      "Integer columns to scale: ['rooms', 'baths', 'm2_cons', 'm2_property', 'floor', 'prop_age', 'luxury_score', 'parking_price', 'consumption', 'emissions', 'amenity_count', 'convenience_score']\n",
      "Boolean columns (already 0/1): ['garage', 'balcony', 'terrace', 'lift', 'AC', 'pool', 'east', 'north', 'south', 'west', 'adosado', 'pareado', 'chalet', 'masia', 'atico', 'duplex', 'estudio', 'piso', 'casa_rustica', 'villa', 'heating', 'trastero', 'fireplace', 'garden', 'wardrobes', 'mobility', 'sea_views', 'nuda', 'ocupada', 'rented', 'new', 'good', 'renovate', 'missing_prop_type', 'missing_prop_age', 'missing_consumption', 'missing_emissions', 'is_house', 'is_apartment', 'is_small', 'top_floor', 'ground_floor', 'is_tiny', 'is_small_apt', 'is_medium', 'is_large', 'is_mansion', 'is_vintage', 'optimal_age', 'has_outdoor', 'has_storage', 'energy_premium', 'energy_penalty', 'is_premium_location', 'bathroom_luxury', 'family_home', 'luxury_property']\n"
     ]
    }
   ],
   "source": [
    "# Prepare features without scaling yet (to prevent data leakage)\n",
    "print(\"Preparing feature matrix...\")\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "for col in boolean_columns: \n",
    "    df_vlc[col] = df_vlc[col].astype(\"int64\")\n",
    "\n",
    "# Create feature matrix and target\n",
    "X = df_vlc.drop(columns=[\"price\"])\n",
    "y = df_vlc[\"price\"]\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Integer columns to scale: {integer_columns}\")\n",
    "print(f\"Boolean columns (already 0/1): {boolean_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a24cc678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying StandardScaler correctly to prevent data leakage...\n",
      "Train shape: (12580, 108), Test shape: (3146, 108)\n",
      "✅ Scaling applied correctly - no data leakage!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../working_data/feature_scaler.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1: Split data FIRST (before any scaling)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, shuffle = True)\n",
    "\n",
    "# STEP 2: Apply StandardScaler properly (fit on train only, transform both)\n",
    "print(\"Applying StandardScaler correctly to prevent data leakage...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler ONLY on training data\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Scale only the integer columns\n",
    "scaler.fit(X_train[integer_columns])  # Learn statistics from training data only\n",
    "X_train_scaled[integer_columns] = scaler.transform(X_train[integer_columns])\n",
    "X_test_scaled[integer_columns] = scaler.transform(X_test[integer_columns])  # Use same transformation\n",
    "\n",
    "# Update the variables\n",
    "X_train = X_train_scaled\n",
    "X_test = X_test_scaled\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "print(\"✅ Scaling applied correctly - no data leakage!\")\n",
    "\n",
    "# Save scaler for future predictions\n",
    "import joblib\n",
    "joblib.dump(scaler, '../working_data/feature_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58cc336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying log transformation to handle price skewness...\n",
      "Original price skewness: 2.249\n",
      "Log-transformed skewness: 0.101 (closer to 0 = better)\n"
     ]
    }
   ],
   "source": [
    "# ACCURACY MAXIMIZATION: Target transformation for better modeling\n",
    "print(\"Applying log transformation to handle price skewness...\")\n",
    "\n",
    "# Check price distribution skewness\n",
    "import scipy.stats as stats\n",
    "skewness = stats.skew(y_train)\n",
    "print(f\"Original price skewness: {skewness:.3f}\")\n",
    "\n",
    "# Apply log transformation (highly effective for price data)\n",
    "y_train_log = np.log(y_train)\n",
    "y_test_log = np.log(y_test)\n",
    "\n",
    "skewness_log = stats.skew(y_train_log)\n",
    "print(f\"Log-transformed skewness: {skewness_log:.3f} (closer to 0 = better)\")\n",
    "\n",
    "# Custom scorer using MAE (but for log-transformed targets)\n",
    "def log_mae(y_true, y_pred):\n",
    "    \"\"\"MAE in original price scale after inverse log transform\"\"\"\n",
    "    return mean_absolute_error(np.exp(y_true), np.exp(y_pred))\n",
    "\n",
    "log_mae_scorer = make_scorer(log_mae, greater_is_better=False)\n",
    "\n",
    "# ENHANCED hyperparameter grids (more comprehensive search)\n",
    "param_grids = {\n",
    "    'LinearRegression': {},  # no params to tune\n",
    "    'Ridge': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1, 10, 50, 100, 500],\n",
    "        'max_iter': [1000, 5000, 10000]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 5, 10, 20, 50],\n",
    "        'max_iter': [5000, 10000, 20000]\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [200, 500, 800],  # More trees for better performance\n",
    "        'max_depth': [None, 15, 25, 35],  # Deeper trees\n",
    "        'min_samples_split': [2, 5, 10],  # Lower for more flexibility\n",
    "        'min_samples_leaf': [1, 2, 4],    # Lower for more flexibility\n",
    "        'max_features': ['sqrt', 'log2', 0.5]  # Feature selection strategies\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [300, 500, 800],\n",
    "        'max_depth': [4, 6, 8, 10],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.15],\n",
    "        'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 0.5],       # L1 regularization\n",
    "        'reg_lambda': [1, 1.5, 2]         # L2 regularization\n",
    "    }\n",
    "}\n",
    "\n",
    "# Enhanced model dictionary with better configurations\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(), \n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'RandomForest': RandomForestRegressor(n_jobs = -1, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_jobs = -1, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "faae43d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models with log-transformed targets and enhanced validation...\n",
      "\n",
      "Tuning LinearRegression...\n",
      "Best params for LinearRegression: {}\n",
      "Best CV score: 47192.9552\n",
      "Training time: 4.8s\n",
      "\n",
      "Tuning Ridge...\n",
      "Best params for LinearRegression: {}\n",
      "Best CV score: 47192.9552\n",
      "Training time: 4.8s\n",
      "\n",
      "Tuning Ridge...\n",
      "Best params for Ridge: {'alpha': 0.01, 'max_iter': 1000}\n",
      "Best CV score: 47186.9361\n",
      "Training time: 4.9s\n",
      "\n",
      "Tuning Lasso...\n",
      "Best params for Ridge: {'alpha': 0.01, 'max_iter': 1000}\n",
      "Best CV score: 47186.9361\n",
      "Training time: 4.9s\n",
      "\n",
      "Tuning Lasso...\n",
      "Best params for Lasso: {'alpha': 0.0001, 'max_iter': 5000}\n",
      "Best CV score: 47910.6657\n",
      "Training time: 143.7s\n",
      "\n",
      "Tuning RandomForest...\n",
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "Best params for Lasso: {'alpha': 0.0001, 'max_iter': 5000}\n",
      "Best CV score: 47910.6657\n",
      "Training time: 143.7s\n",
      "\n",
      "Tuning RandomForest...\n",
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gonzalomartinpenalba/miniconda3/envs/data_env/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 28\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Use log-transformed targets for training\u001b[39;00m\n\u001b[1;32m     19\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m     20\u001b[0m     model, \n\u001b[1;32m     21\u001b[0m     param_grids[name], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandomForest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m grid\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_log)\n\u001b[1;32m     29\u001b[0m best_models[name] \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m     30\u001b[0m training_times[name] \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniconda3/envs/data_env/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/data_env/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1046\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1047\u001b[0m     )\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1051\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/data_env/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1605\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1605\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[0;32m~/miniconda3/envs/data_env/lib/python3.12/site-packages/sklearn/model_selection/_search.py:997\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    993\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    994\u001b[0m         )\n\u001b[1;32m    995\u001b[0m     )\n\u001b[0;32m--> 997\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    998\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    999\u001b[0m         clone(base_estimator),\n\u001b[1;32m   1000\u001b[0m         X,\n\u001b[1;32m   1001\u001b[0m         y,\n\u001b[1;32m   1002\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m   1003\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m   1004\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m   1005\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m   1006\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m   1007\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m   1008\u001b[0m     )\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m )\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1020\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/data_env/lib/python3.12/site-packages/sklearn/utils/parallel.py:82\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     73\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[1;32m     74\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     75\u001b[0m     (\n\u001b[1;32m     76\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     81\u001b[0m )\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config_and_warning_filters)\n",
      "File \u001b[0;32m~/miniconda3/envs/data_env/lib/python3.12/site-packages/joblib/parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/data_env/lib/python3.12/site-packages/joblib/parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/data_env/lib/python3.12/site-packages/joblib/parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[1;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[1;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[1;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[1;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[1;32m   1799\u001b[0m     ):\n\u001b[0;32m-> 1800\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ENHANCED TRAINING WITH BETTER VALIDATION\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "import time\n",
    "\n",
    "# Use stratified validation for more robust hyperparameter selection\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold for faster training\n",
    "\n",
    "best_models = {}\n",
    "training_times = {}\n",
    "\n",
    "print(\"Training models with log-transformed targets and enhanced validation...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nTuning {name}...\")\n",
    "    \n",
    "    # Use log-transformed targets for training\n",
    "    grid = GridSearchCV(\n",
    "        model, \n",
    "        param_grids[name], \n",
    "        scoring=log_mae_scorer, \n",
    "        cv=kfold,\n",
    "        n_jobs=-1,\n",
    "        verbose=1 if name in ['RandomForest', 'XGBoost'] else 0\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_train, y_train_log)\n",
    "    best_models[name] = grid.best_estimator_\n",
    "    training_times[name] = time.time() - start_time\n",
    "    \n",
    "    print(f\"Best params for {name}: {grid.best_params_}\")\n",
    "    print(f\"Best CV score: {-grid.best_score_:.4f}\")\n",
    "    print(f\"Training time: {training_times[name]:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INDIVIDUAL MODEL TRAINING COMPLETED\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a0082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression: MAE = 142196.59, RMSE = 64051783481.80\n",
      "Ridge: MAE = 142276.98, RMSE = 64034671566.63\n",
      "Lasso: MAE = 142247.07, RMSE = 64078042366.38\n",
      "RandomForest: MAE = 85318.94, RMSE = 28548316300.94\n",
      "XGBoost: MAE = 80202.80, RMSE = 26288441344.00\n"
     ]
    }
   ],
   "source": [
    "mae_scores = dict() \n",
    "for name, model in best_models.items():\n",
    "    # Predict on log scale, then transform back\n",
    "    y_pred_log = model.predict(X_test)\n",
    "    y_pred = np.exp(y_pred_log)  # Transform back to original price scale\n",
    "    \n",
    "    # Calculate metrics in original price scale\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mae_scores[name] = mae \n",
    "    rmse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"{name}: MAE = {mae:.2f}, RMSE = {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c5274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315910.70606991707"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vlc[\"price\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab925b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.38780688940931"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "80202.80 / 315910.70606991707 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35cd350",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in best_models.items(): \n",
    "    y_pred_log = model.predict(X_train)\n",
    "    y_pred = np.exp(y_pred_log)\n",
    "    mae = mean_absolute_error(y_train, y_pred)\n",
    "    print(f\"{name}: MAE = {mae:.2f}\")                          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
