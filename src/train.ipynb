{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593fb1f6",
   "metadata": {},
   "source": [
    "#### This file is going to deal with training a model to predict property price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c010fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from xgboost import XGBRegressor \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer \n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import joblib\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1aec76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vlc = pd.read_csv(\"../working_data/properties_vlc_clean2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d99d7a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_vlc.drop(columns = [\"price\"])\n",
    "y = df_vlc[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6eb3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_columns = []\n",
    "one_hot_columns = [\"location_cluster\"]\n",
    "integer_columns = []\n",
    "\n",
    "for column in df_vlc.columns: \n",
    "    if column == \"price\": \n",
    "        continue \n",
    "    elif df_vlc[column].dtype == \"int64\": \n",
    "        if column not in one_hot_columns: \n",
    "            integer_columns.append(column)\n",
    "    elif df_vlc[column].dtype == \"bool\": \n",
    "        boolean_columns.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e64256b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vlc = pd.get_dummies(df_vlc, columns = [\"location_cluster\"], prefix = \"location\", drop_first = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5410768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing feature matrix...\n",
      "Feature matrix shape: (15726, 108)\n",
      "Integer columns to scale: ['rooms', 'baths', 'm2_cons', 'm2_property', 'floor', 'prop_age', 'luxury_score', 'parking_price', 'consumption', 'emissions', 'amenity_count', 'convenience_score']\n",
      "Boolean columns (already 0/1): ['garage', 'balcony', 'terrace', 'lift', 'AC', 'pool', 'east', 'north', 'south', 'west', 'adosado', 'pareado', 'chalet', 'masia', 'atico', 'duplex', 'estudio', 'piso', 'casa_rustica', 'villa', 'heating', 'trastero', 'fireplace', 'garden', 'wardrobes', 'mobility', 'sea_views', 'nuda', 'ocupada', 'rented', 'new', 'good', 'renovate', 'missing_prop_type', 'missing_prop_age', 'missing_consumption', 'missing_emissions', 'is_house', 'is_apartment', 'is_small', 'top_floor', 'ground_floor', 'is_tiny', 'is_small_apt', 'is_medium', 'is_large', 'is_mansion', 'is_vintage', 'optimal_age', 'has_outdoor', 'has_storage', 'energy_premium', 'energy_penalty', 'is_premium_location', 'bathroom_luxury', 'family_home', 'luxury_property']\n"
     ]
    }
   ],
   "source": [
    "# Prepare features without scaling yet (to prevent data leakage)\n",
    "print(\"Preparing feature matrix...\")\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "for col in boolean_columns: \n",
    "    df_vlc[col] = df_vlc[col].astype(\"int64\")\n",
    "\n",
    "# Create feature matrix and target\n",
    "X = df_vlc.drop(columns=[\"price\"])\n",
    "y = df_vlc[\"price\"]\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Integer columns to scale: {integer_columns}\")\n",
    "print(f\"Boolean columns (already 0/1): {boolean_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24cc678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying StandardScaler correctly to prevent data leakage...\n",
      "Train shape: (12580, 108), Test shape: (3146, 108)\n",
      "✅ Scaling applied correctly - no data leakage!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../working_data/feature_scaler.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1: Split data FIRST (before any scaling)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, shuffle = True)\n",
    "\n",
    "# STEP 2: Apply StandardScaler properly (fit on train only, transform both)\n",
    "print(\"Applying StandardScaler correctly to prevent data leakage...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler ONLY on training data\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Scale only the integer columns\n",
    "scaler.fit(X_train[integer_columns])  # Learn statistics from training data only\n",
    "X_train_scaled[integer_columns] = scaler.transform(X_train[integer_columns])\n",
    "X_test_scaled[integer_columns] = scaler.transform(X_test[integer_columns])  # Use same transformation\n",
    "\n",
    "# Update the variables\n",
    "X_train = X_train_scaled\n",
    "X_test = X_test_scaled\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "print(\"✅ Scaling applied correctly - no data leakage!\")\n",
    "\n",
    "# Save scaler for future predictions\n",
    "joblib.dump(scaler, '../working_data/models/feature_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying log transformation to handle price skewness...\n",
      "Original price skewness: 2.249\n",
      "Log-transformed skewness: 0.101 (closer to 0 = better)\n"
     ]
    }
   ],
   "source": [
    "# ACCURACY MAXIMIZATION: Target transformation for better modeling\n",
    "print(\"Applying log transformation to handle price skewness...\")\n",
    "\n",
    "# Check price distribution skewness\n",
    "skewness = stats.skew(y_train)\n",
    "print(f\"Original price skewness: {skewness:.3f}\")\n",
    "\n",
    "# Apply log transformation (highly effective for price data)\n",
    "y_train_log = np.log(y_train)\n",
    "y_test_log = np.log(y_test)\n",
    "\n",
    "skewness_log = stats.skew(y_train_log)\n",
    "print(f\"Log-transformed skewness: {skewness_log:.3f} (closer to 0 = better)\")\n",
    "\n",
    "# Custom scorer using MAE (but for log-transformed targets)\n",
    "def log_mae(y_true, y_pred):\n",
    "    \"\"\"MAE in original price scale after inverse log transform\"\"\"\n",
    "    return mean_absolute_error(np.exp(y_true), np.exp(y_pred))\n",
    "\n",
    "log_mae_scorer = make_scorer(log_mae, greater_is_better=False)\n",
    "\n",
    "# ENHANCED hyperparameter grids (more comprehensive search)\n",
    "param_grids = {\n",
    "    'LinearRegression': {},  # no params to tune\n",
    "    'Ridge': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1, 10, 50, 100, 500],\n",
    "        'max_iter': [1000, 5000, 10000]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 5, 10, 20, 50],\n",
    "        'max_iter': [5000, 10000, 20000]\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [200, 500, 800],  # More trees for better performance\n",
    "        'max_depth': [None, 15, 25, 35],  # Deeper trees\n",
    "        'min_samples_split': [2, 5, 10],  # Lower for more flexibility\n",
    "        'min_samples_leaf': [1, 2, 4],    # Lower for more flexibility\n",
    "        'max_features': ['sqrt', 'log2', 0.5]  # Feature selection strategies\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [300, 500, 800],\n",
    "        'max_depth': [4, 6, 8, 10],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.15],\n",
    "        'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 0.5],       # L1 regularization\n",
    "        'reg_lambda': [1, 1.5, 2]         # L2 regularization\n",
    "    }\n",
    "}\n",
    "\n",
    "# Enhanced model dictionary with better configurations\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(), \n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'RandomForest': RandomForestRegressor(n_jobs = -1, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_jobs = -1, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faae43d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models with log-transformed targets and enhanced validation...\n",
      "\n",
      "Tuning LinearRegression...\n",
      "Best params for LinearRegression: {}\n",
      "Best CV score: 47192.9552\n",
      "Training time: 5.4s\n",
      "\n",
      "Tuning Ridge...\n",
      "Best params for Ridge: {'alpha': 0.01, 'max_iter': 1000}\n",
      "Best CV score: 47186.9361\n",
      "Training time: 6.4s\n",
      "\n",
      "Tuning Lasso...\n",
      "Best params for Lasso: {'alpha': 0.0001, 'max_iter': 5000}\n",
      "Best CV score: 47910.6657\n",
      "Training time: 63.6s\n",
      "\n",
      "Tuning RandomForest...\n",
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "Best params for RandomForest: {'max_depth': 35, 'max_features': 0.5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "Best CV score: 7851.0716\n",
      "Training time: 4206.9s\n",
      "\n",
      "Tuning XGBoost...\n",
      "Fitting 5 folds for each of 6912 candidates, totalling 34560 fits\n",
      "Best params for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 800, 'reg_alpha': 0, 'reg_lambda': 1, 'subsample': 0.7}\n",
      "Best CV score: 3945.6223\n",
      "Training time: 30709.2s\n",
      "\n",
      "==================================================\n",
      "INDIVIDUAL MODEL TRAINING COMPLETED\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Use stratified validation for more robust hyperparameter selection\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold for faster training\n",
    "\n",
    "best_models = {}\n",
    "training_times = {}\n",
    "\n",
    "print(\"Training models with log-transformed targets and enhanced validation...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nTuning {name}...\")\n",
    "    \n",
    "    # Use log-transformed targets for training\n",
    "    grid = GridSearchCV(\n",
    "        model, \n",
    "        param_grids[name], \n",
    "        scoring=log_mae_scorer, \n",
    "        cv=kfold,\n",
    "        n_jobs=-1,\n",
    "        verbose=1 if name in ['RandomForest', 'XGBoost'] else 0\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_train, y_train_log)\n",
    "    best_models[name] = grid.best_estimator_\n",
    "    training_times[name] = time.time() - start_time\n",
    "    \n",
    "    print(f\"Best params for {name}: {grid.best_params_}\")\n",
    "    print(f\"Best CV score: {-grid.best_score_:.4f}\")\n",
    "    print(f\"Training time: {training_times[name]:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INDIVIDUAL MODEL TRAINING COMPLETED\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f66a0082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression: MAE = 48228.42, RMSE = 10875546848.38\n",
      "Ridge: MAE = 48271.59, RMSE = 10861639128.57\n",
      "Lasso: MAE = 49418.45, RMSE = 10742339432.21\n",
      "RandomForest: MAE = 6976.88, RMSE = 487576113.01\n",
      "XGBoost: MAE = 3606.76, RMSE = 125374496.00\n"
     ]
    }
   ],
   "source": [
    "mae_scores = dict() \n",
    "for name, model in best_models.items():\n",
    "    # Predict on log scale, then transform back\n",
    "    y_pred_log = model.predict(X_test)\n",
    "    y_pred = np.exp(y_pred_log)  # Transform back to original price scale\n",
    "    \n",
    "    # Calculate metrics in original price scale\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mae_scores[name] = mae \n",
    "    rmse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"{name}: MAE = {mae:.2f}, RMSE = {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "575c5274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(293791.44633091695)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vlc[\"price\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab925b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1414617899027213"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3606 / 315910.70606991707 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e35cd350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression: MAE = 46545.02\n",
      "Ridge: MAE = 46542.34\n",
      "Lasso: MAE = 47339.35\n",
      "RandomForest: MAE = 2692.50\n",
      "XGBoost: MAE = 957.73\n"
     ]
    }
   ],
   "source": [
    "for name, model in best_models.items(): \n",
    "    y_pred_log = model.predict(X_train)\n",
    "    y_pred = np.exp(y_pred_log)\n",
    "    mae = mean_absolute_error(y_train, y_pred)\n",
    "    print(f\"{name}: MAE = {mae:.2f}\")                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0c27819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving trained models...\n",
      "✅ Saved LinearRegression to ../working_data/models/linearregression.pkl\n",
      "✅ Saved Ridge to ../working_data/models/ridge.pkl\n",
      "✅ Saved Lasso to ../working_data/models/lasso.pkl\n",
      "✅ Saved RandomForest to ../working_data/models/randomforest.pkl\n",
      "✅ Saved XGBoost to ../working_data/models/xgboost.pkl\n",
      "\n",
      "All models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save all trained models to files\n",
    "print(\"Saving trained models...\")\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    filename = f\"../working_data/models/{name.lower()}.pkl\"\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"✅ Saved {name} to {filename}\")\n",
    "\n",
    "print(\"\\nAll models saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c2868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How to load and use saved models for predictions\n",
    "\"\"\"\n",
    "# Load a specific model\n",
    "import joblib\n",
    "\n",
    "# Load the best performing model (e.g., XGBoost)\n",
    "loaded_model = joblib.load('../working_data/model_xgboost.pkl')\n",
    "\n",
    "# Load the scaler (needed for preprocessing new data)\n",
    "scaler = joblib.load('../working_data/feature_scaler.pkl')\n",
    "\n",
    "# For new predictions:\n",
    "# 1. Preprocess new data the same way (one-hot encoding, scaling)\n",
    "# 2. Scale only the integer columns using the saved scaler\n",
    "# 3. Make predictions in log scale, then transform back to original scale\n",
    "\n",
    "# Example prediction workflow:\n",
    "# new_data_scaled = new_data.copy()\n",
    "# new_data_scaled[integer_columns] = scaler.transform(new_data[integer_columns])\n",
    "# predictions_log = loaded_model.predict(new_data_scaled)\n",
    "# predictions = np.exp(predictions_log)  # Transform back to original price scale\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8eba90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
